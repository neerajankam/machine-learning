Statistics
Statistics is the science concerned with developing and studying methods for collecting, analyzing, interpreting and presenting empirical data.

Descriptive Statistics
In the descriptive Statistics, the Data is described in a summarized way. The summarization is done from the sample of the population using different parameters like Mean or standard deviation. Descriptive Statistics are a way of using charts, graphs, and summary measures to organize, represent, and explain a set of Data. 


Inferential Statistics
In the Inferential Statistics, we try to interpret the Meaning of descriptive Statistics. After the Data has been collected, analyzed, and summarised we use Inferential Statistics to describe the Meaning of the collected Data. 


Population
All the members of a group about which you want to draw a conclusion.

Sample
The part of the population selected for analysis.

Variable
A characteristic of an item or an individual that will be analyzed using statistics.

Random variable
A random variable is a variable whose value is unknown or a function that assigns values to each of an experiment's outcomes.

Categorical variable
A categorical variable (also called qualitative variable) refers to a characteristic that can’t be quantifiable. Categorical variables can be either nominal or ordinal.

Nominal variable
A nominal variable is one that describes a name, label or category without natural order. Sex and type of dwelling are examples of nominal variables. 

Ordinal variable
An ordinal variable is a variable whose values are defined by an order relation between the different categories.

Numeric variable
A numeric variable (also called quantitative variable) is a quantifiable characteristic whose values are numbers.

Continuous variable
A variable is said to be continuous if it can assume an infinite number of real values within a given interval.

Discrete variable
As opposed to a continuous variable, a discrete variable can assume only a finite number of real values within a given interval

Event
In probability theory, an event is an outcome or defined collection of outcomes of a random experiment.

Sample space
A sample space is a collection or a set of possible outcomes of a random experiment

Probability Mass, Density function
PMF is a probability measure that gives us probabilities of the possible values for a random variable. Probability mass functions (pmf) are used to describe discrete probability distributions. While probability density functions (pdf) are used to describe continuous probability distributions.

Mean
Mean is the average of all of the numbers.

Median
Median is the middle number, when in order.

Mode
Mode is the most common number.

Range
Range is the largest number minus the smallest number.

Standard Deviation
The Standard Deviation is a measure of how spread out numbers are.

Variance
The average of the squared differences from the Mean.

Distribution: A distribution in statistics is a function that shows the possible values for a variable and how often they occur.

Types of distribution:

1) Bernoulli: A Bernoulli distribution is a discrete probability distribution for a Bernoulli trial — a random experiment that has only two outcomes (usually called a “Success” or a “Failure”).


2) Binomial: A binomial distribution can be thought of as simply the probability of a SUCCESS or FAILURE outcome in an experiment or survey that is repeated multiple times. The binomial is a type of distribution that has two possible outcomes (the prefix “bi” means two, or twice). For example, a coin toss has only two possible outcomes: heads or tails and taking a test could have two possible outcomes: pass or fail.

What Is the Central Limit Theorem (CLT)?
In probability theory, the central limit theorem (CLT) states that the distribution of a sample variable approximates a normal distribution (i.e., a “bell curve”) as the sample size becomes larger, assuming that all samples are identical in size, and regardless of the population's actual distribution shape.

Standardizing a normal distribution
When you standardize a normal distribution, the mean becomes 0 and the standard deviation becomes 1. This allows you to easily calculate the probability of certain values occurring in your distribution, or to compare data sets with different means and standard deviations.

Expected value
Expected value is exactly what you might think it means intuitively: the return you can expect for some kind of action, like how many questions you might get right if you guess on a multiple choice test.


Skewness
Skewness is a statistical measure that assesses the asymmetry of a probability distribution. It quantifies the extent to which the data is skewed or shifted to one side. Positive skewness indicates a longer tail on the right side of the distribution, while negative skewness indicates a longer tail on the left side.

Kurtosis
Kurtosis is a statistical measure used to describe the degree to which scores cluster in the tails or the peak of a frequency distribution.

The peak is the tallest part of the distribution, and the tails are the ends of the distribution.

Quantiles
Quantiles are points in a distribution that relate to the rank order of values in that distribution. For a sample, you can find any quantile by sorting the sample. The middle value of the sorted sample (middle quantile, 50th percentile) is known as the median. The limits are the minimum and maximum values.

Quartiles
Quartiles are three values that split sorted data into four parts, each with an equal number of observations. Quartiles are a type of quantile.

First quartile: Also known as Q1, or the lower quartile. This is the number halfway between the lowest number and the middle number.
Second quartile: Also known as Q2, or the median. This is the middle number halfway between the lowest number and the highest number.
Third quartile: Also known as Q3, or the upper quartile. This is the number halfway between the middle number and the highest number.

Box plot
A box and whisker plot—also called a box plot—displays the five-number summary of a set of data. The five-number summary is the minimum, first quartile, median, third quartile, and maximum. In a box plot, we draw a box from the first quartile to the third quartile. A vertical line goes through the box at the median.

Joint probability 
Joint probability is the probability of two events occurring simultaneously.

Marginal probability 
Marginal probability is the probability of an event irrespective of the outcome of another variable.

Conditional probability
Conditional probability is the probability of one event occurring in the presence of a second event.

Covariance and Correlation
Covariance measures the direction of a relationship between two variables, while correlation measures the strength of that relationship.

Point estimation
Point estimation, in statistics, the process of finding an approximate value of some parameter—such as the mean (average)—of a population from random samples of the population.

Maximum Likelihood estimation
Maximum likelihood estimation (MLE) is an estimation method that allows us to use a sample to estimate the parameters of the probability distribution that generated the sample.

Regularization
Regularization is a way to avoid overfitting by penalizing high-valued regression coefficients. In simple terms, it reduces parameters and shrinks (simplifies) the model.

Maximum A posteriori(MAP) Estimation
Maximum a posteriori estimation, as is stated in its name, maximizes the posterior probability P(A|B) in Bayes’ theorem with respect to the variable A given the variable B is observed.


Margin of Error
A margin of error tells you how many percentage points your results will differ from the real population value. For example, a 95% confidence interval with a 4 percent margin of error means that your statistic will be within 4 percentage points of the real population value 95% of the time.

Confidence interval
A confidence interval, in statistics, refers to the probability that a population parameter will fall between a set of values for a certain proportion of times
